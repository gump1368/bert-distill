# bert-distill
通过fine-tune bert模型获得表现较好的教师网络，再对自定义的学生网络进行“指导学习”。从而学习到bert中丰富的语义知识，提高模型的泛化能力。并且模型很小，等于变相的压缩bert。

## 相关论文
《Distilling Task-Speciﬁc Knowledge from BERT into Simple Neural Networks》 

《Multi-Task Deep Neural Networks for Natural Language Understanding》
